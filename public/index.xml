<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Eric Boittier</title><link>https://www.ericboittier.github.io/</link><atom:link href="https://www.ericboittier.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Eric Boittier</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jul 2024 00:00:00 +0000</lastBuildDate><image><url>https://www.ericboittier.github.io/media/icon_hu9ac327d6012d36714959b8b60a0f0d52_7608_512x512_fill_lanczos_center_3.png</url><title>Eric Boittier</title><link>https://www.ericboittier.github.io/</link></image><item><title>Kernel-based Minimal Distributed Charges: A Conformationally Dependent ESP-Model for Molecular Simulations</title><link>https://www.ericboittier.github.io/publication/kernelmdcm/</link><pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/publication/kernelmdcm/</guid><description/></item><item><title>Flux.1</title><link>https://www.ericboittier.github.io/post/fluxschnell/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/post/fluxschnell/</guid><description>&lt;p>Flux is a generative diffusion model from Blackforest Labs. Diffusion is a process by which particles spread out from a high concentration to a low concentration. The model uses a series of transformations to generate images, starting from a simple noise vector and gradually generating an image over some number of steps.
Here is some output of the model:&lt;/p>
&lt;p>&lt;a href="https://github.com/argmaxinc/DiffusionKit/tree/main" target="_blank" rel="noopener">Diffusion-kit&lt;/a> provides a simple interface to run the model on a macbook,&lt;/p>
&lt;pre>&lt;code class="language-bash">diffusionkit-cli --prompt &amp;quot;prompt&amp;quot; \
--steps 3 --output-path &amp;quot;featured.png&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And in python:&lt;/p>
&lt;pre>&lt;code class="language-python">from diffusionkit.mlx import FluxPipeline
pipeline = FluxPipeline(
shift=1.0,
model_version=&amp;quot;argmaxinc/mlx-FLUX.1-schnell&amp;quot;,
low_memory_mode=True,
a16=True,
w16=True,
)
HEIGHT = 512
WIDTH = 512
NUM_STEPS = 4 # 4 for FLUX.1-schnell, 50 for SD3
CFG_WEIGHT = 0. # for FLUX.1-schnell, 5. for SD3
image, _ = pipeline.generate_image(
&amp;quot;a photo of a cat&amp;quot;,
cfg_weight=CFG_WEIGHT,
num_steps=NUM_STEPS,
latent_size=(HEIGHT // 8, WIDTH // 8),
)
image.save(&amp;quot;image.png&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Flux" srcset="
/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_da0e88cf982901372142a87a3049e92c.webp 400w,
/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_e24e7e401a8090e21304d658e97ac3de.webp 760w,
/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://www.ericboittier.github.io/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_da0e88cf982901372142a87a3049e92c.webp"
width="512"
height="512"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Quite impressive. I thought diffusion would require more steps.
In thermodynamics, diffusion is a slow process, the long-time limit of
how much a system can change.
Certainly, something exciting is going on under the hood.
The model is based on an architecture published on &amp;ldquo;Scaling Rectified Flow Transformers for High-Resolution Image Synthesis&amp;rdquo; by Stability AI (&lt;a href="https://arxiv.org/pdf/2403.03206]" target="_blank" rel="noopener">paper&lt;/a>.&lt;/p>
&lt;h1 id="the-architecture">The architecture&lt;/h1>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="architecture" srcset="
/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_790cbba67b2822c515f7f39430f8518f.webp 400w,
/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_860ec3b4af5dfea78aa0a84bf7df48f2.webp 760w,
/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://www.ericboittier.github.io/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_790cbba67b2822c515f7f39430f8518f.webp"
width="760"
height="586"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Let&amp;rsquo;s see the code:&lt;/p>
&lt;pre>&lt;code class="language-python">class MMDiT(nn.Module):
def __init__(self, config: MMDiTConfig):
super().__init__()
self.config = config
# Input adapters and embeddings
self.x_embedder = LatentImageAdapter(config)
if config.pos_embed_type == PositionalEncoding.LearnedInputEmbedding:
self.x_pos_embedder = LatentImagePositionalEmbedding(config)
self.pre_sdpa_rope = nn.Identity()
elif config.pos_embed_type == PositionalEncoding.PreSDPARope:
self.pre_sdpa_rope = RoPE(
theta=10000,
axes_dim=config.rope_axes_dim,
)
else:
raise ValueError(
f&amp;quot;Unsupported positional encoding type: {config.pos_embed_type}&amp;quot;
)
self.y_embedder = PooledTextEmbeddingAdapter(config)
self.t_embedder = TimestepAdapter(config)
self.context_embedder = nn.Linear(
config.token_level_text_embed_dim,
config.hidden_size,
)
self.multimodal_transformer_blocks = [
MultiModalTransformerBlock(
config,
skip_text_post_sdpa=(i == config.depth_multimodal - 1)
and (config.depth_unified &amp;lt; 1),
)
for i in range(config.depth_multimodal)
]
if config.depth_unified &amp;gt; 0:
self.unified_transformer_blocks = [
UnifiedTransformerBlock(config) for _ in range(config.depth_unified)
]
self.final_layer = FinalLayer(config)
&lt;/code>&lt;/pre>
&lt;h1 id="the-forward-pass">The forward pass:&lt;/h1>
&lt;pre>&lt;code class="language-python"> def __call__(
self,
latent_image_embeddings: mx.array,
token_level_text_embeddings: mx.array,
timestep: mx.array,
) -&amp;gt; mx.array:
batch, latent_height, latent_width, _ = latent_image_embeddings.shape
token_level_text_embeddings = self.context_embedder(token_level_text_embeddings)
if hasattr(self, &amp;quot;x_pos_embedder&amp;quot;):
latent_image_embeddings = self.x_embedder(
latent_image_embeddings
) + self.x_pos_embedder(latent_image_embeddings)
else:
latent_image_embeddings = self.x_embedder(latent_image_embeddings)
latent_image_embeddings = latent_image_embeddings.reshape(
batch, -1, 1, self.config.hidden_size
)
if self.config.pos_embed_type == PositionalEncoding.PreSDPARope:
positional_encodings = self.pre_sdpa_rope(
text_sequence_length=token_level_text_embeddings.shape[1],
latent_image_resolution=(
latent_height // self.config.patch_size,
latent_width // self.config.patch_size,
),
)
else:
positional_encodings = None
# MultiModalTransformer layers
if self.config.depth_multimodal &amp;gt; 0:
for bidx, block in enumerate(self.multimodal_transformer_blocks):
latent_image_embeddings, token_level_text_embeddings = block(
latent_image_embeddings,
token_level_text_embeddings,
timestep,
positional_encodings=positional_encodings,
)
# UnifiedTransformerBlock layers
if self.config.depth_unified &amp;gt; 0:
latent_unified_embeddings = mx.concatenate(
(token_level_text_embeddings, latent_image_embeddings), axis=1
)
for bidx, block in enumerate(self.unified_transformer_blocks):
latent_unified_embeddings = block(
latent_unified_embeddings,
timestep,
positional_encodings=positional_encodings,
)
latent_image_embeddings = latent_unified_embeddings[
:, token_level_text_embeddings.shape[1] :, ...
]
# Final layer
latent_image_embeddings = self.final_layer(
latent_image_embeddings,
timestep,
)
if self.config.patchify_via_reshape:
latent_image_embeddings = self.x_embedder.unpack(
latent_image_embeddings, (latent_height, latent_width)
)
else:
latent_image_embeddings = unpatchify(
latent_image_embeddings,
patch_size=self.config.patch_size,
target_height=latent_height,
target_width=latent_width,
vae_latent_dim=self.config.vae_latent_dim,
)
return latent_image_embeddings
&lt;/code>&lt;/pre>
&lt;h1 id="the-multi-modal-transformer-block">The multi-modal transformer block&lt;/h1>
&lt;pre>&lt;code class="language-python">class MultiModalTransformerBlock(nn.Module):
def __init__(self, config: MMDiTConfig, skip_text_post_sdpa: bool = False):
super().__init__()
self.image_transformer_block = TransformerBlock(config)
self.text_transformer_block = TransformerBlock(
config, skip_post_sdpa=skip_text_post_sdpa
)
sdpa_impl = mx.fast.scaled_dot_product_attention
self.sdpa = partial(sdpa_impl)
self.config = config
self.per_head_dim = config.hidden_size // config.num_heads
def __call__(
self,
latent_image_embeddings: mx.array, # latent image embeddings
token_level_text_embeddings: mx.array, # token-level text embeddings
timestep: mx.array, # pooled text embeddings + timestep embeddings
positional_encodings: mx.array = None, # positional encodings for rope
):
# Prepare multi-modal SDPA inputs
image_intermediates = self.image_transformer_block.pre_sdpa(
latent_image_embeddings,
timestep=timestep,
)
text_intermediates = self.text_transformer_block.pre_sdpa(
token_level_text_embeddings,
timestep=timestep,
)
batch = latent_image_embeddings.shape[0]
def rearrange_for_sdpa(t):
# Target data layout: (batch, head, seq_len, channel)
return t.reshape(
batch, -1, self.config.num_heads, self.per_head_dim
).transpose(0, 2, 1, 3)
if self.config.depth_unified &amp;gt; 0:
multimodal_sdpa_inputs = {
&amp;quot;q&amp;quot;: mx.concatenate(
[text_intermediates[&amp;quot;q&amp;quot;], image_intermediates[&amp;quot;q&amp;quot;]], axis=2
),
&amp;quot;k&amp;quot;: mx.concatenate(
[text_intermediates[&amp;quot;k&amp;quot;], image_intermediates[&amp;quot;k&amp;quot;]], axis=2
),
&amp;quot;v&amp;quot;: mx.concatenate(
[text_intermediates[&amp;quot;v&amp;quot;], image_intermediates[&amp;quot;v&amp;quot;]], axis=2
),
&amp;quot;scale&amp;quot;: 1.0 / np.sqrt(self.per_head_dim),
}
else:
multimodal_sdpa_inputs = {
&amp;quot;q&amp;quot;: rearrange_for_sdpa(
mx.concatenate(
[image_intermediates[&amp;quot;q&amp;quot;], text_intermediates[&amp;quot;q&amp;quot;]], axis=1
)
),
&amp;quot;k&amp;quot;: rearrange_for_sdpa(
mx.concatenate(
[image_intermediates[&amp;quot;k&amp;quot;], text_intermediates[&amp;quot;k&amp;quot;]], axis=1
)
),
&amp;quot;v&amp;quot;: rearrange_for_sdpa(
mx.concatenate(
[image_intermediates[&amp;quot;v&amp;quot;], text_intermediates[&amp;quot;v&amp;quot;]], axis=1
)
),
&amp;quot;scale&amp;quot;: 1.0 / np.sqrt(self.per_head_dim),
}
if self.config.pos_embed_type == PositionalEncoding.PreSDPARope:
assert positional_encodings is not None
multimodal_sdpa_inputs[&amp;quot;q&amp;quot;] = RoPE.apply(
multimodal_sdpa_inputs[&amp;quot;q&amp;quot;], positional_encodings
)
multimodal_sdpa_inputs[&amp;quot;k&amp;quot;] = RoPE.apply(
multimodal_sdpa_inputs[&amp;quot;k&amp;quot;], positional_encodings
)
if self.config.low_memory_mode:
multimodal_sdpa_inputs[
&amp;quot;memory_efficient_threshold&amp;quot;
] = SDPA_FLASH_ATTN_THRESHOLD
# Compute multi-modal SDPA
sdpa_outputs = (
self.sdpa(**multimodal_sdpa_inputs)
.transpose(0, 2, 1, 3)
.reshape(batch, -1, 1, self.config.hidden_size)
)
# Split into image-text sequences for post-SDPA layers
img_seq_len = latent_image_embeddings.shape[1]
txt_seq_len = token_level_text_embeddings.shape[1]
if self.config.depth_unified &amp;gt; 0:
text_sdpa_output = sdpa_outputs[:, :txt_seq_len, :, :]
image_sdpa_output = sdpa_outputs[:, txt_seq_len:, :, :]
else:
image_sdpa_output = sdpa_outputs[:, :img_seq_len, :, :]
text_sdpa_output = sdpa_outputs[:, -txt_seq_len:, :, :]
# Post-SDPA layers
latent_image_embeddings = self.image_transformer_block.post_sdpa(
residual=latent_image_embeddings,
sdpa_output=image_sdpa_output,
**image_intermediates,
)
if self.text_transformer_block.skip_post_sdpa:
# Text token related outputs from the final layer do not impact the model output
token_level_text_embeddings = None
else:
token_level_text_embeddings = self.text_transformer_block.post_sdpa(
residual=token_level_text_embeddings,
sdpa_output=text_sdpa_output,
**text_intermediates,
)
return latent_image_embeddings, token_level_text_embeddings
&lt;/code>&lt;/pre>
&lt;h1 id="the-unified-transformer-block">The unified transformer block&lt;/h1>
&lt;pre>&lt;code class="language-python">class UnifiedTransformerBlock(nn.Module):
def __init__(self, config: MMDiTConfig):
super().__init__()
self.transformer_block = TransformerBlock(
config,
num_modulation_params=3 if config.parallel_mlp_for_unified_blocks else 6,
parallel_mlp=config.parallel_mlp_for_unified_blocks,
)
sdpa_impl = mx.fast.scaled_dot_product_attention
self.sdpa = partial(sdpa_impl)
self.config = config
self.per_head_dim = config.hidden_size // config.num_heads
def __call__(
self,
latent_unified_embeddings: mx.array, # latent image embeddings
timestep: mx.array, # pooled text embeddings + timestep embeddings
positional_encodings: mx.array = None, # positional encodings for rope
):
# Prepare multi-modal SDPA inputs
intermediates = self.transformer_block.pre_sdpa(
latent_unified_embeddings,
timestep=timestep,
)
batch = latent_unified_embeddings.shape[0]
def rearrange_for_sdpa(t):
# Target data layout: (batch, head, seq_len, channel)
return t.reshape(
batch, -1, self.config.num_heads, self.per_head_dim
).transpose(0, 2, 1, 3)
multimodal_sdpa_inputs = {
&amp;quot;q&amp;quot;: intermediates[&amp;quot;q&amp;quot;],
&amp;quot;k&amp;quot;: intermediates[&amp;quot;k&amp;quot;],
&amp;quot;v&amp;quot;: intermediates[&amp;quot;v&amp;quot;],
&amp;quot;scale&amp;quot;: 1.0 / np.sqrt(self.per_head_dim),
}
if self.config.pos_embed_type == PositionalEncoding.PreSDPARope:
assert positional_encodings is not None
multimodal_sdpa_inputs[&amp;quot;q&amp;quot;] = RoPE.apply(
multimodal_sdpa_inputs[&amp;quot;q&amp;quot;], positional_encodings
)
multimodal_sdpa_inputs[&amp;quot;k&amp;quot;] = RoPE.apply(
multimodal_sdpa_inputs[&amp;quot;k&amp;quot;], positional_encodings
)
if self.config.low_memory_mode:
multimodal_sdpa_inputs[
&amp;quot;memory_efficient_threshold&amp;quot;
] = SDPA_FLASH_ATTN_THRESHOLD
# Compute multi-modal SDPA
sdpa_outputs = (
self.sdpa(**multimodal_sdpa_inputs)
.transpose(0, 2, 1, 3)
.reshape(batch, -1, 1, self.config.hidden_size)
)
# o_proj and mlp.fc2 uses the same bias, remove mlp.fc2 bias
self.transformer_block.mlp.fc2.bias = self.transformer_block.mlp.fc2.bias * 0.0
# Post-SDPA layers
latent_unified_embeddings = self.transformer_block.post_sdpa(
residual=latent_unified_embeddings,
sdpa_output=sdpa_outputs,
**intermediates,
)
return latent_unified_embeddings
&lt;/code>&lt;/pre>
&lt;h1 id="the-final-layer">The final layer&lt;/h1>
&lt;pre>&lt;code class="language-python">class FinalLayer(nn.Module):
def __init__(self, config: MMDiTConfig):
super().__init__()
self.norm_final = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
self.linear = nn.Linear(
config.hidden_size,
(config.patch_size**2) * config.vae_latent_dim,
)
self.adaLN_modulation = nn.Sequential(
nn.SiLU(),
nn.Linear(config.hidden_size, 2 * config.hidden_size),
)
def __call__(
self,
latent_image_embeddings: mx.array,
timestep: mx.array,
) -&amp;gt; mx.array:
if timestep.size &amp;gt; 1:
timestep = timestep[0]
modulation_params = self._modulation_params[timestep.item()]
shift, residual_scale = mx.split(modulation_params, 2, axis=-1)
latent_image_embeddings = affine_transform(
latent_image_embeddings,
shift=shift,
residual_scale=residual_scale,
norm_module=self.norm_final,
)
return self.linear(latent_image_embeddings)
&lt;/code>&lt;/pre></description></item><item><title>From Juptyer to Blog</title><link>https://www.ericboittier.github.io/post/test-notebook/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/post/test-notebook/</guid><description>&lt;p>The following post was made
by creating a Jupyter notebook and converting it to a blog post,
using the nbconvert tool.&lt;/p>
&lt;pre>&lt;code class="language-bash">jupyter nbconvert --to markdown Test\ Notebook\ Blog\ Post.ipynb --NbConvertApp.output_files_dir=.
&lt;/code>&lt;/pre>
&lt;p>The command above will convert the notebook to Markdown and save it in the same directory as the notebook.
Adding the usual Hugo front matter to the markdown file will allow it to be rendered as a blog post.
Assuming you already have an index.md file with front matter, something like:&lt;/p>
&lt;pre>&lt;code class="language-bash">cat 'Test Notebook Blog Post.md' | tee -a index.md
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;will do the trick!&lt;/p>
&lt;h1 id="test-notebook-blog-post">Test Notebook Blog Post&lt;/h1>
&lt;pre>&lt;code class="language-python">import matplotlib.pyplot as plt
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">import numpy as np
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s do something a bit random!&lt;/p>
&lt;pre>&lt;code class="language-python">X = np.random.rand(100).reshape(10,10)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">plt.imshow(X)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>&amp;lt;matplotlib.image.AxesImage at 0x1248bd4b0&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./Test%20Notebook%20Blog%20Post_5_1.png" alt="png" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>Fortran is fast. Profile your code to make it faster!</title><link>https://www.ericboittier.github.io/post/profiling-fortan/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/post/profiling-fortan/</guid><description>&lt;h2 id="all-about-fortran">All About Fortran&lt;/h2>
&lt;h3 id="what-is-fortran">What is Fortran?&lt;/h3>
&lt;p>Fortran is a dinosaur code language. It was released around 65 years ago (date of writing) by John Backus and IBM for electric computing machines powered by the fossilized remains of dinosaurs who lived around 65 million years ago.&lt;/p>
&lt;p>&amp;lsquo;Too lazy&amp;rsquo; to write assembly, Backus wrote this compiled imperative language to save himself time when composing complicated mathematical formulas, giving rise to the &amp;lsquo;Formula Translation&amp;rsquo; language or FORTRAN. Nowadays, FORTRAN is considered too verbose by modern programming standards. Although much slower, Python might be considered the new Formula Translation language. Routines like the Fast Fourier Transform have been reduced to one line (np.fft()), where the number of lines of pure FORTRAN and assembly code needed are around 1 to 2 orders of magnitude longer, respectively.&lt;/p>
&lt;h3 id="why-fortran">Why Fortran?&lt;/h3>
&lt;p>Fortran is fast. Many legacy applications rely on Fortran for reasons related to speed and compatibility.&lt;/p>
&lt;h2 id="the-good-type-of-profiling">The Good Type of Profiling&lt;/h2>
&lt;h3 id="how-can-we-test-the-speed-of-our-code">How can we test the speed of our code?&lt;/h3>
&lt;p>Profiling is a strategy to monitor the performance of one&amp;rsquo;s code.
Results often show the time spent in individual routines, the number of calls, as well as the order in which routines have been accessed.&lt;/p>
&lt;h2 id="profiling-fortran-with-gprof">Profiling Fortran with gprof&lt;/h2>
&lt;p>Fortran must first be compiled with the following, additional flags:&lt;/p>
&lt;pre>&lt;code class="language-bash">... -pg fcheck=bounds,mem
&lt;/code>&lt;/pre>
&lt;p>The &amp;lsquo;fcheck&amp;rsquo; option allows the compiler to produce warnings for attempts at accessing undefined memory, etc.&lt;/p>
&lt;p>Once compiled, run your code as usual:&lt;/p>
&lt;pre>&lt;code class="language-bash">./yourcode
&lt;/code>&lt;/pre>
&lt;p>A file named &amp;lsquo;gmon.out&amp;rsquo; will be created in the current directory.
To see the results of the profiling&lt;/p>
&lt;pre>&lt;code class="language-bash">gprof -l -b ./yourcode &amp;gt; gprof.log
&lt;/code>&lt;/pre>
&lt;p>The -g and -l flags in the compilation and profiling steps, respectively, allow for a line by line analysis of time spent during computation. Without these options, the profiler will show total time spent in each subroutine.&lt;/p></description></item><item><title>Notes on Baysian Optmization</title><link>https://www.ericboittier.github.io/post/notes/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/post/notes/</guid><description>&lt;h2 id="surrogate-models-to-the-rescue">Surrogate Models to the Rescue&lt;/h2>
&lt;p>If you have a cost function that is too expensive to evaluate, you should check out Bayesian Optimization.&lt;/p>
&lt;p>The idea is to use a surrogate model to approximate the cost function and then use this model to find the best point to evaluate next.&lt;/p>
&lt;p>The most common surrogate model is a Gaussian Process (GP), which is a distribution over functions. The GP is defined by its mean function $m(x)$ and covariance function $k(x, x&amp;rsquo;)$:&lt;/p>
&lt;p>$$f(x) \sim \mathcal{GP}(m(x), k(x, x&amp;rsquo;))$$&lt;/p>
&lt;p>The GP is updated with the new data point and then used to find the next point to evaluate. This is typically done by maximizing an acquisition function, such as the Expected Improvement (EI):&lt;/p>
&lt;p>$$EI(x) = \mathbb{E}[\max(f(x) - f(x^+), 0)]$$&lt;/p>
&lt;p>where $f(x^+)$ is the current best observed value.&lt;/p></description></item><item><title>Molecular Dynamics with Conformationally Dependent, Distributed Charges</title><link>https://www.ericboittier.github.io/publication/fmdcm/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/publication/fmdcm/</guid><description/></item><item><title>Impact of the Characteristics of Quantum Chemical Databases on Machine Learning Prediction of Tautomerization Energies</title><link>https://www.ericboittier.github.io/publication/mldatabase/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/publication/mldatabase/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>Supplementary notes can be added here, including &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">code, math, and images&lt;/a>.&lt;/p></description></item><item><title>Transfer Learning to CCSD(T): Accurate Anharmonic Frequencies from Machine Learning Models</title><link>https://www.ericboittier.github.io/publication/mlccsdt/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/publication/mlccsdt/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>Supplementary notes can be added here, including &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">code, math, and images&lt;/a>.&lt;/p></description></item><item><title>Assessing Molecular Docking Tools to Guide Targeted Drug Discovery of CD38 Inhibitors</title><link>https://www.ericboittier.github.io/publication/docking/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/publication/docking/</guid><description/></item><item><title>GlycoTorch Vina: Docking Designed and Tested for Glycosaminoglycans</title><link>https://www.ericboittier.github.io/publication/glycotorch/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/publication/glycotorch/</guid><description/></item><item><title>Slides</title><link>https://www.ericboittier.github.io/slides/example/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/slides/example/</guid><description>&lt;h1 id="create-slides-in-markdown-with-wowchemy">Create slides in Markdown with Wowchemy&lt;/h1>
&lt;p>&lt;a href="https://wowchemy.com/" target="_blank" rel="noopener">Wowchemy&lt;/a> | &lt;a href="https://owchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="features">Features&lt;/h2>
&lt;ul>
&lt;li>Efficiently write slides in Markdown&lt;/li>
&lt;li>3-in-1: Create, Present, and Publish your slides&lt;/li>
&lt;li>Supports speaker notes&lt;/li>
&lt;li>Mobile friendly slides&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="controls">Controls&lt;/h2>
&lt;ul>
&lt;li>Next: &lt;code>Right Arrow&lt;/code> or &lt;code>Space&lt;/code>&lt;/li>
&lt;li>Previous: &lt;code>Left Arrow&lt;/code>&lt;/li>
&lt;li>Start: &lt;code>Home&lt;/code>&lt;/li>
&lt;li>Finish: &lt;code>End&lt;/code>&lt;/li>
&lt;li>Overview: &lt;code>Esc&lt;/code>&lt;/li>
&lt;li>Speaker notes: &lt;code>S&lt;/code>&lt;/li>
&lt;li>Fullscreen: &lt;code>F&lt;/code>&lt;/li>
&lt;li>Zoom: &lt;code>Alt + Click&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://github.com/hakimel/reveal.js#pdf-export" target="_blank" rel="noopener">PDF Export&lt;/a>: &lt;code>E&lt;/code>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="code-highlighting">Code Highlighting&lt;/h2>
&lt;p>Inline code: &lt;code>variable&lt;/code>&lt;/p>
&lt;p>Code block:&lt;/p>
&lt;pre>&lt;code class="language-python">porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
print(&amp;quot;Eating...&amp;quot;)
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="math">Math&lt;/h2>
&lt;p>In-line math: $x + y = z$&lt;/p>
&lt;p>Block math:&lt;/p>
&lt;p>$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p>
&lt;hr>
&lt;h2 id="fragments">Fragments&lt;/h2>
&lt;p>Make content appear incrementally&lt;/p>
&lt;pre>&lt;code>{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code>&lt;/pre>
&lt;p>Press &lt;code>Space&lt;/code> to play!&lt;/p>
&lt;span class="fragment " >
One
&lt;/span>
&lt;span class="fragment " >
&lt;strong>Two&lt;/strong>
&lt;/span>
&lt;span class="fragment " >
Three
&lt;/span>
&lt;hr>
&lt;p>A fragment can accept two optional parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>class&lt;/code>: use a custom style (requires definition in custom CSS)&lt;/li>
&lt;li>&lt;code>weight&lt;/code>: sets the order in which a fragment appears&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="speaker-notes">Speaker Notes&lt;/h2>
&lt;p>Add speaker notes to your presentation&lt;/p>
&lt;pre>&lt;code class="language-markdown">{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code>&lt;/pre>
&lt;p>Press the &lt;code>S&lt;/code> key to view the speaker notes!&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Only the speaker can read these notes&lt;/li>
&lt;li>Press &lt;code>S&lt;/code> key to view&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;hr>
&lt;h2 id="themes">Themes&lt;/h2>
&lt;ul>
&lt;li>black: Black background, white text, blue links (default)&lt;/li>
&lt;li>white: White background, black text, blue links&lt;/li>
&lt;li>league: Gray background, white text, blue links&lt;/li>
&lt;li>beige: Beige background, dark text, brown links&lt;/li>
&lt;li>sky: Blue background, thin dark text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;ul>
&lt;li>night: Black background, thick white text, orange links&lt;/li>
&lt;li>serif: Cappuccino background, gray text, brown links&lt;/li>
&lt;li>simple: White background, black text, blue links&lt;/li>
&lt;li>solarized: Cream-colored background, dark green text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;section data-noprocess data-shortcode-slide
data-background-image="/media/boards.jpg"
>
&lt;h2 id="custom-slide">Custom Slide&lt;/h2>
&lt;p>Customize the slide style and background&lt;/p>
&lt;pre>&lt;code class="language-markdown">{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h2 id="custom-css-example">Custom CSS Example&lt;/h2>
&lt;p>Let&amp;rsquo;s make headers navy colored.&lt;/p>
&lt;p>Create &lt;code>assets/css/reveal_custom.css&lt;/code> with:&lt;/p>
&lt;pre>&lt;code class="language-css">.reveal section h1,
.reveal section h2,
.reveal section h3 {
color: navy;
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;h1 id="questions">Questions?&lt;/h1>
&lt;p>&lt;a href="https://github.com/wowchemy/wowchemy-hugo-modules/discussions" target="_blank" rel="noopener">Ask&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p></description></item><item><title/><link>https://www.ericboittier.github.io/arxiv-tinder/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/arxiv-tinder/</guid><description/></item><item><title>All Blog Posts</title><link>https://www.ericboittier.github.io/all-posts/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/all-posts/</guid><description/></item><item><title>Example Project</title><link>https://www.ericboittier.github.io/project/example/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/project/example/</guid><description>&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item><item><title>Eric Boittier</title><link>https://www.ericboittier.github.io/authors/admin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.ericboittier.github.io/authors/admin/</guid><description>&lt;p>A blog about machine learning for the physical sciences, with a focus on molecular dynamics simulations.&lt;/p>
&lt;!-- I'm a Ph.D. candidate working under the supervision of Prof. Markus Meuwly and Prof. O. Anatole von Lilienfeld. -->
&lt;!--
&lt;i class="fas fa-download pr-1 fa-fw">&lt;/i> Download my &lt;a href="https://www.ericboittier.github.io/uploads/EricBoittier_CV.pdf" target="_blank">resumé&lt;/a>. --></description></item></channel></rss>