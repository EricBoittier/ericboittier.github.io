<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning | Eric Boittier</title><link>https://ericboittier@githubpages.io/category/machine-learning/</link><atom:link href="https://ericboittier@githubpages.io/category/machine-learning/index.xml" rel="self" type="application/rss+xml"/><description>machine learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 17 Aug 2022 00:00:00 +0000</lastBuildDate><image><url>https://ericboittier@githubpages.io/media/icon_hu9ac327d6012d36714959b8b60a0f0d52_7608_512x512_fill_lanczos_center_3.png</url><title>machine learning</title><link>https://ericboittier@githubpages.io/category/machine-learning/</link></image><item><title>From Juptyer to Blog</title><link>https://ericboittier@githubpages.io/post/test-notebook/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://ericboittier@githubpages.io/post/test-notebook/</guid><description>&lt;h1 id="test-notebook-blog-post">Test Notebook Blog Post&lt;/h1>
&lt;pre>&lt;code class="language-python">import matplotlib.pyplot as plt
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">import numpy as np
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s do something a bit random!&lt;/p>
&lt;pre>&lt;code class="language-python">X = np.random.rand(100).reshape(10,10)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">plt.imshow(X)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>&amp;lt;matplotlib.image.AxesImage at 0x1248bd4b0&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./Test%20Notebook%20Blog%20Post_5_1.png" alt="png" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;pre>&lt;code class="language-python">
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">
&lt;/code>&lt;/pre></description></item><item><title>Notes on Baysian Optmization</title><link>https://ericboittier@githubpages.io/post/notes/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://ericboittier@githubpages.io/post/notes/</guid><description>&lt;h2 id="surrogate-models-to-the-rescue">Surrogate Models to the Rescue&lt;/h2>
&lt;p>If you have a cost function that is too expensive to evaluate, you should check out Bayesian Optimization.&lt;/p>
&lt;p>The idea is to use a surrogate model to approximate the cost function and then use this model to find the best point to evaluate next.&lt;/p>
&lt;p>The most common surrogate model is a Gaussian Process (GP), which is a distribution over functions. The GP is defined by its mean function $m(x)$ and covariance function $k(x, x&amp;rsquo;)$:&lt;/p>
&lt;p>$$f(x) \sim \mathcal{GP}(m(x), k(x, x&amp;rsquo;))$$&lt;/p>
&lt;p>The GP is updated with the new data point and then used to find the next point to evaluate. This is typically done by maximizing an acquisition function, such as the Expected Improvement (EI):&lt;/p>
&lt;p>$$EI(x) = \mathbb{E}[\max(f(x) - f(x^+), 0)]$$&lt;/p>
&lt;p>where $f(x^+)$ is the current best observed value.&lt;/p></description></item></channel></rss>