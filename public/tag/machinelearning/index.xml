<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machinelearning | Eric Boittier</title><link>https://ericboittier@githubpages.io/tag/machinelearning/</link><atom:link href="https://ericboittier@githubpages.io/tag/machinelearning/index.xml" rel="self" type="application/rss+xml"/><description>machinelearning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 17 Aug 2023 00:00:00 +0000</lastBuildDate><image><url>https://ericboittier@githubpages.io/media/icon_hu9ac327d6012d36714959b8b60a0f0d52_7608_512x512_fill_lanczos_center_3.png</url><title>machinelearning</title><link>https://ericboittier@githubpages.io/tag/machinelearning/</link></image><item><title>From Juptyer to Blog</title><link>https://ericboittier@githubpages.io/post/test-notebook/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://ericboittier@githubpages.io/post/test-notebook/</guid><description>&lt;p>The following post was made
by creating a Jupyter notebook and converting it to a blog post,
using the nbconvert tool.&lt;/p>
&lt;pre>&lt;code class="language-bash">jupyter nbconvert --to markdown Test\ Notebook\ Blog\ Post.ipynb --NbConvertApp.output_files_dir=.
&lt;/code>&lt;/pre>
&lt;p>The command above will convert the notebook to Markdown and save it in the same directory as the notebook.
Adding the usual Hugo front matter to the markdown file will allow it to be rendered as a blog post.
Assuming you already have an index.md file with front matter, something like:&lt;/p>
&lt;pre>&lt;code class="language-bash">cat 'Test Notebook Blog Post.md' | tee -a index.md
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;will do the trick!&lt;/p>
&lt;h1 id="test-notebook-blog-post">Test Notebook Blog Post&lt;/h1>
&lt;pre>&lt;code class="language-python">import matplotlib.pyplot as plt
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">import numpy as np
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s do something a bit random!&lt;/p>
&lt;pre>&lt;code class="language-python">X = np.random.rand(100).reshape(10,10)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">plt.imshow(X)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>&amp;lt;matplotlib.image.AxesImage at 0x1248bd4b0&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./Test%20Notebook%20Blog%20Post_5_1.png" alt="png" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>Notes on Baysian Optmization</title><link>https://ericboittier@githubpages.io/post/notes/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>https://ericboittier@githubpages.io/post/notes/</guid><description>&lt;h2 id="surrogate-models-to-the-rescue">Surrogate Models to the Rescue&lt;/h2>
&lt;p>If you have a cost function that is too expensive to evaluate, you should check out Bayesian Optimization.&lt;/p>
&lt;p>The idea is to use a surrogate model to approximate the cost function and then use this model to find the best point to evaluate next.&lt;/p>
&lt;p>The most common surrogate model is a Gaussian Process (GP), which is a distribution over functions. The GP is defined by its mean function $m(x)$ and covariance function $k(x, x&amp;rsquo;)$:&lt;/p>
&lt;p>$$f(x) \sim \mathcal{GP}(m(x), k(x, x&amp;rsquo;))$$&lt;/p>
&lt;p>The GP is updated with the new data point and then used to find the next point to evaluate. This is typically done by maximizing an acquisition function, such as the Expected Improvement (EI):&lt;/p>
&lt;p>$$EI(x) = \mathbb{E}[\max(f(x) - f(x^+), 0)]$$&lt;/p>
&lt;p>where $f(x^+)$ is the current best observed value.&lt;/p></description></item></channel></rss>