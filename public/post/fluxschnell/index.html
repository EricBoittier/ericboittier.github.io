<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.4.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Eric Boittier"><meta name=description content="Generative AI on a macbook."><link rel=alternate hreflang=en-us href=https://ericboittier.github.io/post/fluxschnell/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.d3fbabf3e02f0a40f84592dd992c35ca.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu9ac327d6012d36714959b8b60a0f0d52_7608_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu9ac327d6012d36714959b8b60a0f0d52_7608_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ericboittier.github.io/post/fluxschnell/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="Eric Boittier"><meta property="og:url" content="https://ericboittier.github.io/post/fluxschnell/"><meta property="og:title" content="Flux.1 | Eric Boittier"><meta property="og:description" content="Generative AI on a macbook."><meta property="og:image" content="https://ericboittier.github.io/post/fluxschnell/featured.png"><meta property="twitter:image" content="https://ericboittier.github.io/post/fluxschnell/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-08-17T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-17T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://ericboittier.github.io/post/fluxschnell/"},"headline":"Flux.1","image":["https://ericboittier.github.io/post/fluxschnell/featured.png"],"datePublished":"2023-08-17T00:00:00Z","dateModified":"2023-08-17T00:00:00Z","author":{"@type":"Person","name":"Eric Boittier"},"publisher":{"@type":"Organization","name":"Eric Boittier","logo":{"@type":"ImageObject","url":"https://ericboittier.github.io/media/icon_hu9ac327d6012d36714959b8b60a0f0d52_7608_192x192_fill_lanczos_center_3.png"}},"description":"Generative AI on a macbook."}</script><title>Flux.1 | Eric Boittier</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=2a224d24bb013949c70a0e8ee67a57b7><script src=/js/wowchemy-init.min.9e9fa4b2ccd29f9edfae161d14d2effd.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Eric Boittier</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Eric Boittier</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/all-posts/><span>Archive</span></a></li><li class=nav-item><a class=nav-link href=/arxiv-tinder/><span>arxivtinder</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Flux.1</h1><p class=page-subtitle>Image model from Blackforest Labs</p><div class=article-metadata><div><span>admin</span></div><span class=article-date>Aug 17, 2023</span>
<span class=middot-divider></span>
<span class=article-reading-time>6 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/generative/>generative</a></span></div></div><div class="article-header container featured-image-wrapper mt-4 mb-4" style=max-width:512px;max-height:512px><div style=position:relative><img src=/post/fluxschnell/featured_huce07940aa0bd532616862e6d5adee120_331911_1200x2500_fit_q75_h2_lanczos_3.webp width=512 height=512 alt class=featured-image>
<span class=article-header-caption>Image credit: diffusionkit)</span></div></div><div class=article-container><div class=article-style><p>Flux is a generative diffusion model from Blackforest Labs. Diffusion is a process by which particles spread out from a high concentration to a low concentration. The model uses a series of transformations to generate images, starting from a simple noise vector and gradually generating an image over some number of steps.
Here is some output of the model:</p><p><a href=https://github.com/argmaxinc/DiffusionKit/tree/main target=_blank rel=noopener>Diffusion-kit</a> provides a simple interface to run the model on a macbook,</p><pre><code class=language-bash>diffusionkit-cli --prompt &quot;prompt&quot; \
--steps 3  --output-path &quot;featured.png&quot;
</code></pre><p>And in python:</p><pre><code class=language-python>from diffusionkit.mlx import FluxPipeline
pipeline = FluxPipeline(
  shift=1.0,
  model_version=&quot;argmaxinc/mlx-FLUX.1-schnell&quot;,
  low_memory_mode=True,
  a16=True,
  w16=True,
)

HEIGHT = 512
WIDTH = 512
NUM_STEPS = 4  #  4 for FLUX.1-schnell, 50 for SD3
CFG_WEIGHT = 0. # for FLUX.1-schnell, 5. for SD3

image, _ = pipeline.generate_image(
  &quot;a photo of a cat&quot;,
  cfg_weight=CFG_WEIGHT,
  num_steps=NUM_STEPS,
  latent_size=(HEIGHT // 8, WIDTH // 8),
)

image.save(&quot;image.png&quot;)
</code></pre><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=Flux srcset="/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_da0e88cf982901372142a87a3049e92c.webp 400w,
/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_e24e7e401a8090e21304d658e97ac3de.webp 760w,
/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/fluxschnell/image_hu7f8f19a1c5b64af49ec81e894680faf0_321997_da0e88cf982901372142a87a3049e92c.webp width=512 height=512 loading=lazy data-zoomable></div></div></figure></p><p>Quite impressive. I thought diffusion would require more steps.
In thermodynamics, diffusion is a slow process, the long-time limit of
how much a system can change.
Certainly, something exciting is going on under the hood.
The model is based on an architecture published on &ldquo;Scaling Rectified Flow Transformers for High-Resolution Image Synthesis&rdquo; by Stability AI (<a href=https://arxiv.org/pdf/2403.03206] target=_blank rel=noopener>paper</a>).</p><p>Let&rsquo;s look a bit deeper in the code:</p><p>The text is transformed into an embedding, which is used to condition the image generation. The function &rsquo;encode_text&rsquo; is used to encode the text into an embedding.</p><pre><code class=language-python> conditioning, pooled_conditioning = self.encode_text(
            text, cfg_weight, negative_text
        )
        mx.eval(conditioning)
        mx.eval(pooled_conditioning)
</code></pre><p>The diffusion steps take place inside the <code>FluxPipeline</code> class, in the <code>generate_image</code> method.
The specific function inside this method is:</p><pre><code class=language-python>def denoise_latents(
self,
conditioning,
pooled_conditioning,
num_steps: int = 2,
cfg_weight: float = 0.0,
latent_size: Tuple[int] = (64, 64),
seed=None,
image_path: Optional[str] = None,
denoise: float = 1.0,
): # Set the PRNG state
seed = int(time.time()) if seed is None else seed
logger.info(f&quot;Seed: {seed}&quot;)
mx.random.seed(seed)
x_T = self.get_empty_latent(*latent_size)
if image_path is None:
    denoise = 1.0
else:
    x_T = self.encode_image_to_latents(image_path, seed=seed)
    x_T = self.latent_format.process_in(x_T)
noise = self.get_noise(seed, x_T)
sigmas = self.get_sigmas(self.sampler, num_steps)
sigmas = sigmas[int(num_steps * (1 - denoise)) :]
extra_args = {
    &quot;conditioning&quot;: conditioning,
    &quot;cfg_weight&quot;: cfg_weight,
    &quot;pooled_conditioning&quot;: pooled_conditioning,
}
noise_scaled = self.sampler.noise_scaling(
    sigmas[0], noise, x_T, self.max_denoise(sigmas)
)
latent, iter_time = sample_euler(
    CFGDenoiser(self), noise_scaled, sigmas, extra_args=extra_args
)

latent = self.latent_format.process_out(latent)

return latent, iter_time
</code></pre><p>The function &lsquo;decode_latents_to_image&rsquo; is used to decode the latent representation to an image. It calls the decoder and then the clip module to get the final image.
The code:</p><pre><code class=language-python>    def decode_latents_to_image(self, x_t):
        x = self.decoder(x_t)
        x = mx.clip(x / 2 + 0.5, 0, 1)
        return x
</code></pre><h1 id=the-architecture>The architecture</h1><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=architecture srcset="/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_790cbba67b2822c515f7f39430f8518f.webp 400w,
/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_860ec3b4af5dfea78aa0a84bf7df48f2.webp 760w,
/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/fluxschnell/figure_hud725ec3a009e98800b3eb1d4b4fa0d69_422974_790cbba67b2822c515f7f39430f8518f.webp width=760 height=586 loading=lazy data-zoomable></div></div></figure></p><pre><code class=language-python>class MMDiT(nn.Module):
def __init__(self, config: MMDiTConfig):
        super().__init__()
        self.config = config

        # Input adapters and embeddings
        self.x_embedder = LatentImageAdapter(config)

        if config.pos_embed_type == PositionalEncoding.LearnedInputEmbedding:
            self.x_pos_embedder = LatentImagePositionalEmbedding(config)
            self.pre_sdpa_rope = nn.Identity()
        elif config.pos_embed_type == PositionalEncoding.PreSDPARope:
            self.pre_sdpa_rope = RoPE(
                theta=10000,
                axes_dim=config.rope_axes_dim,
            )
        else:
            raise ValueError(
                f&quot;Unsupported positional encoding type: {config.pos_embed_type}&quot;
            )

        self.y_embedder = PooledTextEmbeddingAdapter(config)
        self.t_embedder = TimestepAdapter(config)
        self.context_embedder = nn.Linear(
            config.token_level_text_embed_dim,
            config.hidden_size,
        )

        self.multimodal_transformer_blocks = [
            MultiModalTransformerBlock(
                config,
                skip_text_post_sdpa=(i == config.depth_multimodal - 1)
                and (config.depth_unified &lt; 1),
            )
            for i in range(config.depth_multimodal)
        ]

        if config.depth_unified &gt; 0:
            self.unified_transformer_blocks = [
                UnifiedTransformerBlock(config) for _ in range(config.depth_unified)
            ]

        self.final_layer = FinalLayer(config)
</code></pre><h1 id=the-forward-pass>The forward pass:</h1><pre><code class=language-python>    def __call__(
        self,
        latent_image_embeddings: mx.array,
        token_level_text_embeddings: mx.array,
        timestep: mx.array,
    ) -&gt; mx.array:
        batch, latent_height, latent_width, _ = latent_image_embeddings.shape
        token_level_text_embeddings = self.context_embedder(token_level_text_embeddings)

        if hasattr(self, &quot;x_pos_embedder&quot;):
            latent_image_embeddings = self.x_embedder(
                latent_image_embeddings
            ) + self.x_pos_embedder(latent_image_embeddings)
        else:
            latent_image_embeddings = self.x_embedder(latent_image_embeddings)

        latent_image_embeddings = latent_image_embeddings.reshape(
            batch, -1, 1, self.config.hidden_size
        )

        if self.config.pos_embed_type == PositionalEncoding.PreSDPARope:
            positional_encodings = self.pre_sdpa_rope(
                text_sequence_length=token_level_text_embeddings.shape[1],
                latent_image_resolution=(
                    latent_height // self.config.patch_size,
                    latent_width // self.config.patch_size,
                ),
            )
        else:
            positional_encodings = None

        # MultiModalTransformer layers
        if self.config.depth_multimodal &gt; 0:
            for bidx, block in enumerate(self.multimodal_transformer_blocks):
                latent_image_embeddings, token_level_text_embeddings = block(
                    latent_image_embeddings,
                    token_level_text_embeddings,
                    timestep,
                    positional_encodings=positional_encodings,
                )

        # UnifiedTransformerBlock layers
        if self.config.depth_unified &gt; 0:
            latent_unified_embeddings = mx.concatenate(
                (token_level_text_embeddings, latent_image_embeddings), axis=1
            )

            for bidx, block in enumerate(self.unified_transformer_blocks):
                latent_unified_embeddings = block(
                    latent_unified_embeddings,
                    timestep,
                    positional_encodings=positional_encodings,
                )

            latent_image_embeddings = latent_unified_embeddings[
                :, token_level_text_embeddings.shape[1] :, ...
            ]

        # Final layer
        latent_image_embeddings = self.final_layer(
            latent_image_embeddings,
            timestep,
        )

        if self.config.patchify_via_reshape:
            latent_image_embeddings = self.x_embedder.unpack(
                latent_image_embeddings, (latent_height, latent_width)
            )
        else:
            latent_image_embeddings = unpatchify(
                latent_image_embeddings,
                patch_size=self.config.patch_size,
                target_height=latent_height,
                target_width=latent_width,
                vae_latent_dim=self.config.vae_latent_dim,
            )
        return latent_image_embeddings
</code></pre><p>The model is composed of several blocks. The first block is the input adapter and embeddings.
The first loop is the multi-modal transformer blocks, and the second loop is the unified transformer blocks, before the final layer.</p><h1 id=the-multi-modal-transformer-block>The multi-modal transformer block</h1><pre><code class=language-python>class MultiModalTransformerBlock(nn.Module):
    def __init__(self, config: MMDiTConfig, skip_text_post_sdpa: bool = False):
        super().__init__()
        self.image_transformer_block = TransformerBlock(config)
        self.text_transformer_block = TransformerBlock(
            config, skip_post_sdpa=skip_text_post_sdpa
        )

        sdpa_impl = mx.fast.scaled_dot_product_attention
        self.sdpa = partial(sdpa_impl)

        self.config = config
        self.per_head_dim = config.hidden_size // config.num_heads

    def __call__(
        self,
        latent_image_embeddings: mx.array,  # latent image embeddings
        token_level_text_embeddings: mx.array,  # token-level text embeddings
        timestep: mx.array,  # pooled text embeddings + timestep embeddings
        positional_encodings: mx.array = None,  # positional encodings for rope
    ):
        # Prepare multi-modal SDPA inputs
        image_intermediates = self.image_transformer_block.pre_sdpa(
            latent_image_embeddings,
            timestep=timestep,
        )

        text_intermediates = self.text_transformer_block.pre_sdpa(
            token_level_text_embeddings,
            timestep=timestep,
        )

        batch = latent_image_embeddings.shape[0]

        def rearrange_for_sdpa(t):
            # Target data layout: (batch, head, seq_len, channel)
            return t.reshape(
                batch, -1, self.config.num_heads, self.per_head_dim
            ).transpose(0, 2, 1, 3)

        if self.config.depth_unified &gt; 0:
            multimodal_sdpa_inputs = {
                &quot;q&quot;: mx.concatenate(
                    [text_intermediates[&quot;q&quot;], image_intermediates[&quot;q&quot;]], axis=2
                ),
                &quot;k&quot;: mx.concatenate(
                    [text_intermediates[&quot;k&quot;], image_intermediates[&quot;k&quot;]], axis=2
                ),
                &quot;v&quot;: mx.concatenate(
                    [text_intermediates[&quot;v&quot;], image_intermediates[&quot;v&quot;]], axis=2
                ),
                &quot;scale&quot;: 1.0 / np.sqrt(self.per_head_dim),
            }
        else:
            multimodal_sdpa_inputs = {
                &quot;q&quot;: rearrange_for_sdpa(
                    mx.concatenate(
                        [image_intermediates[&quot;q&quot;], text_intermediates[&quot;q&quot;]], axis=1
                    )
                ),
                &quot;k&quot;: rearrange_for_sdpa(
                    mx.concatenate(
                        [image_intermediates[&quot;k&quot;], text_intermediates[&quot;k&quot;]], axis=1
                    )
                ),
                &quot;v&quot;: rearrange_for_sdpa(
                    mx.concatenate(
                        [image_intermediates[&quot;v&quot;], text_intermediates[&quot;v&quot;]], axis=1
                    )
                ),
                &quot;scale&quot;: 1.0 / np.sqrt(self.per_head_dim),
            }

        if self.config.pos_embed_type == PositionalEncoding.PreSDPARope:
            assert positional_encodings is not None
            multimodal_sdpa_inputs[&quot;q&quot;] = RoPE.apply(
                multimodal_sdpa_inputs[&quot;q&quot;], positional_encodings
            )
            multimodal_sdpa_inputs[&quot;k&quot;] = RoPE.apply(
                multimodal_sdpa_inputs[&quot;k&quot;], positional_encodings
            )

        if self.config.low_memory_mode:
            multimodal_sdpa_inputs[
                &quot;memory_efficient_threshold&quot;
            ] = SDPA_FLASH_ATTN_THRESHOLD

        # Compute multi-modal SDPA
        sdpa_outputs = (
            self.sdpa(**multimodal_sdpa_inputs)
            .transpose(0, 2, 1, 3)
            .reshape(batch, -1, 1, self.config.hidden_size)
        )

        # Split into image-text sequences for post-SDPA layers
        img_seq_len = latent_image_embeddings.shape[1]
        txt_seq_len = token_level_text_embeddings.shape[1]

        if self.config.depth_unified &gt; 0:
            text_sdpa_output = sdpa_outputs[:, :txt_seq_len, :, :]
            image_sdpa_output = sdpa_outputs[:, txt_seq_len:, :, :]
        else:
            image_sdpa_output = sdpa_outputs[:, :img_seq_len, :, :]
            text_sdpa_output = sdpa_outputs[:, -txt_seq_len:, :, :]

        # Post-SDPA layers
        latent_image_embeddings = self.image_transformer_block.post_sdpa(
            residual=latent_image_embeddings,
            sdpa_output=image_sdpa_output,
            **image_intermediates,
        )
        if self.text_transformer_block.skip_post_sdpa:
            # Text token related outputs from the final layer do not impact the model output
            token_level_text_embeddings = None
        else:
            token_level_text_embeddings = self.text_transformer_block.post_sdpa(
                residual=token_level_text_embeddings,
                sdpa_output=text_sdpa_output,
                **text_intermediates,
            )

        return latent_image_embeddings, token_level_text_embeddings
</code></pre><h1 id=the-unified-transformer-block>The unified transformer block</h1><pre><code class=language-python>class UnifiedTransformerBlock(nn.Module):
    def __init__(self, config: MMDiTConfig):
        super().__init__()
        self.transformer_block = TransformerBlock(
            config,
            num_modulation_params=3 if config.parallel_mlp_for_unified_blocks else 6,
            parallel_mlp=config.parallel_mlp_for_unified_blocks,
        )

        sdpa_impl = mx.fast.scaled_dot_product_attention
        self.sdpa = partial(sdpa_impl)

        self.config = config
        self.per_head_dim = config.hidden_size // config.num_heads

    def __call__(
        self,
        latent_unified_embeddings: mx.array,  # latent image embeddings
        timestep: mx.array,  # pooled text embeddings + timestep embeddings
        positional_encodings: mx.array = None,  # positional encodings for rope
    ):
        # Prepare multi-modal SDPA inputs
        intermediates = self.transformer_block.pre_sdpa(
            latent_unified_embeddings,
            timestep=timestep,
        )

        batch = latent_unified_embeddings.shape[0]

        def rearrange_for_sdpa(t):
            # Target data layout: (batch, head, seq_len, channel)
            return t.reshape(
                batch, -1, self.config.num_heads, self.per_head_dim
            ).transpose(0, 2, 1, 3)

        multimodal_sdpa_inputs = {
            &quot;q&quot;: intermediates[&quot;q&quot;],
            &quot;k&quot;: intermediates[&quot;k&quot;],
            &quot;v&quot;: intermediates[&quot;v&quot;],
            &quot;scale&quot;: 1.0 / np.sqrt(self.per_head_dim),
        }

        if self.config.pos_embed_type == PositionalEncoding.PreSDPARope:
            assert positional_encodings is not None
            multimodal_sdpa_inputs[&quot;q&quot;] = RoPE.apply(
                multimodal_sdpa_inputs[&quot;q&quot;], positional_encodings
            )
            multimodal_sdpa_inputs[&quot;k&quot;] = RoPE.apply(
                multimodal_sdpa_inputs[&quot;k&quot;], positional_encodings
            )

        if self.config.low_memory_mode:
            multimodal_sdpa_inputs[
                &quot;memory_efficient_threshold&quot;
            ] = SDPA_FLASH_ATTN_THRESHOLD

        # Compute multi-modal SDPA
        sdpa_outputs = (
            self.sdpa(**multimodal_sdpa_inputs)
            .transpose(0, 2, 1, 3)
            .reshape(batch, -1, 1, self.config.hidden_size)
        )

        # o_proj and mlp.fc2 uses the same bias, remove mlp.fc2 bias
        self.transformer_block.mlp.fc2.bias = self.transformer_block.mlp.fc2.bias * 0.0

        # Post-SDPA layers
        latent_unified_embeddings = self.transformer_block.post_sdpa(
            residual=latent_unified_embeddings,
            sdpa_output=sdpa_outputs,
            **intermediates,
        )

        return latent_unified_embeddings
</code></pre><h1 id=the-final-layer>The final layer</h1><p>The final transformation of the latent image embeddings.</p><pre><code class=language-python>class FinalLayer(nn.Module):
    def __init__(self, config: MMDiTConfig):
        super().__init__()
        self.norm_final = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.linear = nn.Linear(
            config.hidden_size,
            (config.patch_size**2) * config.vae_latent_dim,
        )
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(config.hidden_size, 2 * config.hidden_size),
        )

    def __call__(
        self,
        latent_image_embeddings: mx.array,
        timestep: mx.array,
    ) -&gt; mx.array:
        if timestep.size &gt; 1:
            timestep = timestep[0]
        modulation_params = self._modulation_params[timestep.item()]

        shift, residual_scale = mx.split(modulation_params, 2, axis=-1)
        latent_image_embeddings = affine_transform(
            latent_image_embeddings,
            shift=shift,
            residual_scale=residual_scale,
            norm_module=self.norm_final,
        )
        return self.linear(latent_image_embeddings)
</code></pre></div><div class=article-tags><a class="badge badge-light" href=/tag/code/>code</a>
<a class="badge badge-light" href=/tag/machinelearning/>machinelearning</a></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26cbef546776b4d6032d1ea3dafadab.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.1ed7170a9173a61f55662bbf6cc675b2.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js type=module></script></body></html>